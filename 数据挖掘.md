# 数据挖掘

1. 为什么需要数据挖掘 **Why Data Mining**
   - 为从大量数据中获取有价值的信息
   - 数据大量增长，需要功能强大和通用的工具，从海量数据中获得有价值的信息。
   - 数据的来源多种多样，维度增加
   - 社会工作有数据挖掘的需求
2. 什么是数据挖掘？ **what is Data mining**
   - 从大量数据中提取有价值的模式和知识‘
3. **KDD**流程
   1. 数据清洗
   2. 数据集成
   3. 数据选择、转换
   4. 数据挖掘
   5. 模式评估
   6. 知识呈现
4. KDD应用
   - 表征、判别
   - 关联
   - 分类
   - 聚类
   - 异常值
   - 趋势



+++

1. **Apriori（频繁模式挖掘）**

> Apriori算法详解 https://zhuanlan.zhihu.com/p/341882260
>
> **关联分析**（一）：频繁项集及规则产生https://blog.csdn.net/huguozhiengr/article/details/82869591

-  **频繁项集**：满足最小支持度的所有项集，称作频繁项集。

-  **频繁项集性质**：1、频繁项集的所有非空子集也为频繁项集；2、若A项集不是频繁项集，则其他项集或事务与A项集的并集也不是频繁项集）

- **强关联规则表示**同时满足最小支持度和最小置信度阈值要求的所有关联规则。(**大于等于**)

2. **FP-Tree**

https://www.cnblogs.com/pinard/p/6307064.html

+++

1. 统计量计算

 [统计学——中位数、众数](https://www.cnblogs.com/swxj/p/6624433.html)  https://www.cnblogs.com/swxj/p/6624433.html

箱线图   https://zhuanlan.zhihu.com/p/235345817

相似性计算 https://zhuanlan.zhihu.com/p/347253198

数据 https://zhuanlan.zhihu.com/p/138410347

数据相似性测度 https://zhuanlan.zhihu.com/p/54032594



2. 卡方检验 https://www.itcast.cn/news/20200807/1524429563.shtml
3. 



# 数据预处理过程

1. 数据清洗
   - 缺失值填充(**删除元组**, 人工填写，自动填写，决策树、贝叶斯决策)
   - 异常值处理
   - 平滑噪音数据
   - 解决数据不一致性
2. 数据集成:**整合多个数据库的数据**
3. 减少数据
   - 降维
   - 减少数据数量
   - 数据压缩
4. 数据正规化、离散化。



# 数据

1. 标称数据

即：属性值之间并无逻辑关系（**例如,进行编码后最大值，最小值没有意义**），例如颜色（红蓝黄绿）

**相似性测度**:
$$
d(i, j) = \frac{len(p) - m}{len(p)} \\
p:对象属性个数
m：对象i、j属性相同值的个数
$$

2. 二元属性
   - 对称二元属性

$$
d(i, j) = \frac{r + s}{q + r + s + t}
$$

​			非对称二元属性
$$
d(i, j) = \frac{r + s}{q + r + s}
$$


# 卡方检验

**实际观测**

|        | 下棋 | 不下其 | 和   |
| ------ | ---- | ------ | ---- |
| 像科幻 | 250  | 200    | 450  |
| 不像   | 50   | 1000   | 1050 |
| 综合   | 300  | 1200   | 1500 |



**期望观测**

| 期望值 | 下棋 | 不下琪 | 和   |
| ------ | ---- | ------ | ---- |
| 像科幻 | 90   | 360    | 450  |
| 不像   | 210  | 840    | 1050 |
| 综合   | 300  | 1200   | 1500 |

$$
期望值计算\\
(下棋 \& 像科幻) \quad \frac{450 * 300}{1500} = 90 \\
(不下其 \& 像科幻) \quad \frac{450 * 1200}{1500} = 360
$$

**计算公式**
$$
X^2 = \sum_{i=1}^{4}{\frac{(S_i - e_i)^2}{e_i}} \\
s_i 实际值 \quad e_i 期望值
$$
**即**：
$$
X^2= \frac{(250-90)^2}{90} +  \frac{(200-360)^2}{360}
		+  \frac{(50-210)^2}{210} +  \frac{(1000-840)^2}{840}
$$
$X^2$越大，则证明两者相关性就越大



+++



# 频繁项集挖掘

## Apriori 算法

- 支持度

$$
support(x \Rightarrow y) = \frac{p(xy)}{p(all \_ data)}
$$

即`xy`出现频率

- 置信度

$$
confidence(x \Rightarrow y) = p(y|x) = \frac{p(xy)}{p(x)}
$$

**参数**: 

- 最小支持度 $min \_ Support$
- 最小置信度$min\_Confidence$



**当某个集合支持度$\geq min\_Support$ **时，该集合为频繁项集

当某个频繁项集置信度$\geq min\_Confidence$时为强关联规则  



**重要性质**

1. 若某个集合不为频繁项集，则包含该集合的超集都不为频繁项集
2. 若某个集合为频繁项集，则它的子集都为频繁项集



计算过程:

![img](image/数据挖掘/1042406-20170117161036255-1753157633-1669045578082-3.png)

$C_i: 候选集 \quad L_i:i项频繁项集$



优点:

1. 算法容易理解
2. 进行了剪枝操作，减少了一定的计算量

缺点:

1. 每形成一个新的频繁集合，都要扫描一次原数据集，**时间复杂度极高**

+++



## FP-Tree

**FP-Tree**：腌